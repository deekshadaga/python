pyspark in google colab

#pyspark and findspark installation
!pip3 install findspark pyspark
import findspark
!wget http://13.234.66.67/summer19/bigdata/spark-2.4.0-bin-hadoop2.7.tgz
!tar -xvzf spark-2.4.0-bin-hadoop2.7.tgz
import findspark
#loading driver of pyspark
findspark.init('spark-2.4.0-bin-hadoop2.7')
#now we can import pyspark 
from pyspark import SparkContext
# calling sparkcontext
sc=SparkContext()
# now loading a file from url to create first rdd
!wget http://3.90.19.144/data.txt
frdd=sc.textFile('data.txt')
type(frdd)

#lets create a list
x=["hello","world","this","is","spark"]
type(x)
#transforming list data into rdd
srdd=sc.parallelize(x)
type(srdd)
#some basic operations in spark
frdd.first() #to print very first line from first datasource
srdd.first() #to print very first line from second datasource
#to print few lines
frdd.take(3)
frdd.take(50)
#to count words we need to apply split in all data
datasplit=frdd.flatMap(lambda line:line.split(" ")) 
# here datasplit is rdd second
#for line in frdd that is :
#datasplit=[line for line in frdd line.split(" ")]
# now time for maping words as per second rdd
mapping=datasplit.map(lambda word:(word,1))
#time for reducing RDD
w_count=mapping.reduceByKey(lambda a,b:a+b)
#print
w_count.take(50)
import matplotlib.pyplot as plt
w=[]
c=[]
for i in w_count.take(50):
  w.append(i[0])
  c.append(i[1])
  # now plotting the graph
plt.figure(figsize=(10,12))
plt.xlabel("words")
plt.ylabel("count")
plt.bar(w,c)
#now as pie chart
plt.pie(c,labels=w,autopct='%.2f')
